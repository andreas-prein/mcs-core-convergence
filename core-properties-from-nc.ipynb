{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core-properties-from-nc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis program reads in simulated and observed data and calculates\\ndraft properties as seen from a virtual profiler, 2D slizes, and 3D slizes\\n\\nAuthor: Andreas F Prein\\nemail:  prein@ucar.edu\\ndate:   Oct. 6th, 2023\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This program reads in simulated and observed data and calculates\n",
    "draft properties as seen from a virtual profiler, 2D slizes, and 3D slizes\n",
    "\n",
    "Author: Andreas F Prein\n",
    "email:  prein@ucar.edu\n",
    "date:   Oct. 6th, 2023\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import rrule\n",
    "import datetime\n",
    "import glob\n",
    "from netCDF4 import Dataset\n",
    "import sys, traceback\n",
    "import dateutil.parser as dparser\n",
    "import string\n",
    "from pdb import set_trace as stop\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "# from mpl_toolkits import basemap\n",
    "# import ESMF\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "import pylab as plt\n",
    "import random\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy\n",
    "import shapefile\n",
    "import matplotlib.path as mplPath\n",
    "from matplotlib.patches import Polygon as Polygon2\n",
    "# Cluster specific modules\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "from scipy.ndimage import gaussian_filter\n",
    "# import seaborn as sns\n",
    "# import metpy.calc as mpcalc\n",
    "import shapefile as shp\n",
    "import sys \n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn\n",
    "# from mpl_toolkits.basemap import Basemap, cm\n",
    "import wrf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draft_functions import core_2d_properties, core_3d_properties, watersheding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input sectionsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DX = ['500M','250M','125M']\n",
    "# DT = [2,1,0.5]\n",
    "# dx_m = [500,250,125] \n",
    "\n",
    "# DX = ['12KM']\n",
    "# DT = [32]\n",
    "# dx_m = [12000] \n",
    "\n",
    "DX_all = ['4KM','2KM','1KM','500M','250M','125M']\n",
    "DT_all = [16,8,4,2,1,0.5]\n",
    "dx_m_all = [4000,2000,1000,500,250,125] \n",
    "\n",
    "skip_ml = '_ml-scip' # ['', '_ml-scip'] if yes --> 1 km arround the ML will be set to zero\n",
    "\n",
    "dx_name = '125M' #str(sys.argv[1])\n",
    "si      = 17 #int(sys.argv[2])\n",
    "\n",
    "DX = [dx_name]\n",
    "dx_sel =  DX_all.index(dx_name)\n",
    "\n",
    "\n",
    "DT = [DT_all[dx_sel]]\n",
    "dx_m = [dx_m_all[dx_sel]] \n",
    "\n",
    "SIM_All = ['mao_20140401_15:00:00_',\n",
    "        'mao_20140917_17:00:00_',\n",
    "        'mao_20141004_13:00:00_',\n",
    "        'mao_20141018_14:00:00_',\n",
    "        'mao_20141117_18:00:00_',\n",
    "        'mao_20141210_14:00:00_',\n",
    "        'mao_20150328_15:00:00_',\n",
    "        'mao_20150412_12:00:00_',\n",
    "        'mao_20150621_14:00:00_',\n",
    "        'mao_20151106_12:00:00_',\n",
    "        'sgp_20120531_04:00:00_',\n",
    "        'sgp_20120615_07:00:00_',\n",
    "        'sgp_20130509_07:00:00_',\n",
    "        'sgp_20130605_09:00:00_',\n",
    "        'sgp_20130617_07:00:00_',\n",
    "        'sgp_20140602_04:00:00_',\n",
    "        'sgp_20140605_12:00:00_',\n",
    "        'sgp_20140612_06:00:00_',\n",
    "        'sgp_20140628_16:00:00_',\n",
    "        'sgp_20140710_10:00:00_']\n",
    "\n",
    "# si = SIM_All.index('sgp_20120615_07:00:00_')\n",
    "# SIM = SIM_All[si]\n",
    "# Site = SIM[:3].upper()\n",
    "SIM = SIM_All[si]\n",
    "Site = SIM[:3].upper()\n",
    "\n",
    "\n",
    "vwp_dir = '/glade/campaign/mmm/c3we/mingge/WRF_DOE/virtual_profiler_NetCDF/'\n",
    "wrfout_dir = '/glade/campaign/mmm/c3we/mingge/WRF_DOE/'\n",
    "\n",
    "outdir = '/glade/campaign/mmm/c3we/prein/Papers/2023_Conv-Cores/data/core_stats'+skip_ml+'/'+SIM_All[si]+'/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "if np.isin(DX[0], ('500M','250M','125M')) == True:\n",
    "    subkm = True\n",
    "else:\n",
    "    subkm = False\n",
    "           \n",
    "\n",
    "VARS = ['WW','QS','QR','QC','QG','QV','TK','P'] # virtual profiler variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Virtual Profiler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/glade/derecho/scratch/prein/tmp/ipykernel_61170/72078607.py\u001b[0m(37)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     35 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mskip_ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'_ml-scip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m            \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 37 \u001b[0;31m            \u001b[0mlev_fl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVARS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m273.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m            \u001b[0mfl_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlev_fl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlev_fl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m            \u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfl_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:23<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "grDATA = {}\n",
    "dx = 0\n",
    "if subkm == True:\n",
    "    vwp_file = vwp_dir + SIM_All[si][:-1] + '/' + DX[dx] + '/'+ SIM_All[si]+ '_'+ DX[dx] +'_Loc01.nc'\n",
    "    ncfile = Dataset(vwp_file)\n",
    "    height = np.squeeze(ncfile.variables[\"Height\"])\n",
    "    time = np.squeeze(ncfile.variables['Time'])\n",
    "    ncfile.close()\n",
    "\n",
    "    # from datetime import timedelta\n",
    "    from datetime import datetime, timedelta\n",
    "    StartDay = datetime(int(SIM[4:8]), int(SIM[8:10]), int(SIM[10:12]), int(SIM[13:15])) - timedelta(hours=6)\n",
    "    StopDay = StartDay  + timedelta(hours=12)\n",
    "    if DT[dx] < 1:\n",
    "        rgdTimeMCS = pd.date_range(StartDay, end=StopDay, freq=str(DT[dx]*1000)+'ms')[:-1]\n",
    "    else:\n",
    "        rgdTimeMCS = pd.date_range(StartDay, end=StopDay, freq=str(DT[dx])+'s')\n",
    "    rgdTimeMCS = rgdTimeMCS[:len(time)]\n",
    "\n",
    "    DATA = np.zeros((len(time),len(height), 20, len(VARS))); DATA[:] = np.nan\n",
    "    lat_vs = np.zeros((20)); lat_vs[:] = np.nan\n",
    "    lon_vs = np.copy(lat_vs)\n",
    "\n",
    "    for lo in tqdm(range(20)):  \n",
    "        loc = str(lo+1).zfill(2)\n",
    "        vwp_file = vwp_dir + SIM_All[si][:-1] + '/' + DX[dx] + '/'+ SIM_All[si]+ '_'+ DX[dx] +'_Loc' + loc + '.nc'\n",
    "        ncfile = Dataset(vwp_file)\n",
    "\n",
    "        lat_vs[lo] = ncfile.getncattr('profiler latitude')\n",
    "        lon_vs[lo] = ncfile.getncattr('profiler longitude')\n",
    "        for va in range(len(VARS)):\n",
    "            # print('    '+VARS[va])\n",
    "            DATA[:,:,lo,va] = np.squeeze(ncfile.variables[VARS[va]])\n",
    "            \n",
    "        if skip_ml == '_ml-scip':\n",
    "            lev_fl = (height >= 3000) & (height <= 5000)\n",
    "            #lev_fl = np.argmin(np.abs(np.nanmean(DATA[:,:,7,VARS.index('TK')]-273.15, axis=0)))\n",
    "            #fl_layer = (height - height[lev_fl] >= -500) & (height - height[lev_fl] <= 500)\n",
    "            DATA[:,fl_layer,:,:] = np.nan\n",
    "            \n",
    "        grDATA[DX[dx]] = DATA\n",
    "        \n",
    "    if DATA.shape[0] != len(rgdTimeMCS):\n",
    "        if DT[dx] < 1:\n",
    "            StopDay = StartDay + timedelta(milliseconds=DATA.shape[0]*DT[dx]*1000)\n",
    "            rgdTimeMCS = pd.date_range(StartDay, end=StopDay, freq=str(DT[dx]*1000)+'ms')[:-1]\n",
    "        else:\n",
    "            StopDay = StartDay + timedelta(seconds=DATA.shape[0]*DT[dx])\n",
    "            rgdTimeMCS = pd.date_range(StartDay, end=StopDay, freq=str(DT[dx])+'s')[:-1]\n",
    "else:\n",
    "    # we load the virtual profiler from the 500 m simulation\n",
    "    # to define the MCS and its timing\n",
    "    vwp_file = vwp_dir + SIM_All[si][:-1] + '/250M/'+ SIM_All[si]+ '_250M_Loc01.nc'\n",
    "    ncfile = Dataset(vwp_file)\n",
    "    height = np.squeeze(ncfile.variables[\"Height\"])\n",
    "    time = np.squeeze(ncfile.variables['Time'])\n",
    "    tk = np.squeeze(ncfile.variables['TK'])\n",
    "    ncfile.close()\n",
    "\n",
    "    # from datetime import timedelta\n",
    "    from datetime import datetime, timedelta\n",
    "    StartDay = datetime(int(SIM[4:8]), int(SIM[8:10]), int(SIM[10:12]), int(SIM[13:15])) - timedelta(hours=24)\n",
    "    StopDay = StartDay  + timedelta(hours=36)\n",
    "    rgdTimeMCS = pd.date_range(StartDay, end=StopDay, freq=str(DT[dx])+'s') #'500ms')\n",
    "    rgdTimeMCS = rgdTimeMCS[:len(time)]\n",
    "\n",
    "    DATA = np.zeros((len(time),len(height), 20, len(VARS))); DATA[:] = np.nan\n",
    "    lat_vs = np.zeros((20)); lat_vs[:] = np.nan\n",
    "    lon_vs = np.copy(lat_vs)\n",
    "    \n",
    "    if skip_ml == '_ml-scip':\n",
    "        stop()\n",
    "        lev_fl = np.argmin(np.abs(np.nanmean(tk-273.15, axis=0)))\n",
    "        fl_layer = (height - height[lev_fl] >= -500) & (height - height[lev_fl] <= 500)\n",
    "        DATA[:,fl_layer,:,:] = np.nan\n",
    "        \n",
    "    for lo in tqdm(range(20)):  \n",
    "        loc = str(lo+1).zfill(2)\n",
    "        vwp_file = vwp_dir + SIM_All[si][:-1] + '/250M/'+ SIM_All[si]+ '_250M_Loc' + loc + '.nc'\n",
    "        ncfile = Dataset(vwp_file)\n",
    "\n",
    "        lat_vs[lo] = ncfile.getncattr('profiler latitude')\n",
    "        lon_vs[lo] = ncfile.getncattr('profiler longitude')\n",
    "        # for va in range(len(VARS)):\n",
    "        #     # print('    '+VARS[va])\n",
    "        #     DATA[:,:,lo,va] = np.squeeze(ncfile.variables[VARS[va]])\n",
    "        # grDATA[DX[dx]] = DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dBZ at 20th model leve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subkm == True:\n",
    "    wrfout_files = np.sort(glob.glob(wrfout_dir+DX[dx]+'/Thomson_YSU/'+SIM+DX[dx]+'/wrfout_d02*'))\n",
    "else:\n",
    "    if DX[dx] != '12KM':\n",
    "        wrfout_files = np.sort(glob.glob(wrfout_dir+DX[dx]+'/Thomson_YSU/'+SIM+'L'+DX[dx][0]+'/wrfout_d01*'))\n",
    "    else:\n",
    "        wrfout_files = np.sort(glob.glob(wrfout_dir+DX[dx]+'/Thomson_YSU_0/'+SIM+'L'+DX[dx][:2]+'/wrfout_d01*'))\n",
    "    # only focus the analysis on 18 - 30 hours \n",
    "    wrfout_files = wrfout_files[18*6+1:18*6+1+12*6]\n",
    "    \n",
    "ncfile = Dataset(wrfout_files[0])\n",
    "lat = np.squeeze(ncfile.variables[\"XLAT\"])\n",
    "lon = np.squeeze(ncfile.variables['XLONG'])\n",
    "ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbz = np.zeros((len(wrfout_files), lat.shape[0], lat.shape[1])); dbz[:] = np.nan\n",
    "# tb = np.copy(dbz)\n",
    "for tt in tqdm(range(len(wrfout_files))):\n",
    "    ncfile = Dataset(wrfout_files[tt])\n",
    "    dbz[tt,:,:] = np.max(np.squeeze(ncfile.variables[\"REFL_10CM\"][0,:,:,:]), axis=0)\n",
    "    # olr = np.squeeze(ncfile.variables[\"OLR\"][0,:,:])\n",
    "    # a = 1.228\n",
    "    # b = -1.106e-3\n",
    "    # sigma = 5.67e-8 # W m^-2 K^-4\n",
    "    # tf = (olr/sigma)**0.25\n",
    "    # tb[tt,:,:] = (-a + np.sqrt(a**2 + 4*b*tf))/(2*b)\n",
    "    ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time vector for wrfout files\n",
    "import wrf\n",
    "ncfile = Dataset(wrfout_files[0])\n",
    "start_wrfout = wrf.extract_times(ncfile,0)\n",
    "ncfile = Dataset(wrfout_files[-1])\n",
    "stop_wrfout = wrf.extract_times(ncfile,0)\n",
    "\n",
    "time_wrfout = pd.date_range(start_wrfout, end=stop_wrfout, freq='10min') #'500ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track high-reflectivity areas and determine their speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idbz_threshold = 30\n",
    "MinObjVolume = (40 * 1000)**2 # m2\n",
    "iSmooth = 16000\n",
    "\n",
    "\n",
    "iSmoothGC=int(iSmooth/dx_m[dx])\n",
    "dbz_smooth=scipy.ndimage.uniform_filter(dbz[:,:,:],[1,iSmoothGC,iSmoothGC])\n",
    "\n",
    "# threshold the dbz\n",
    "rgiTH_dbz=(dbz_smooth >= idbz_threshold)\n",
    "rgrdbz_thresholded=np.copy(dbz)\n",
    "rgrdbz_thresholded[rgiTH_dbz == False]=0\n",
    "rgiObj_Struct=np.zeros((3,3,3)); rgiObj_Struct[1,:,:]=1\n",
    "rgiObjectsUD, nr_objectsUD = ndimage.label(rgiTH_dbz,structure=rgiObj_Struct)\n",
    "\n",
    "# remove too small objects\n",
    "Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "dbz_objects=np.copy(rgiObjectsUD); dbz_objects[:]=0\n",
    "ii=0\n",
    "jj=0\n",
    "for ob in tqdm(range(len(Objects))):\n",
    "    obj_act = np.copy(rgiObjectsUD[Objects[ob]])\n",
    "    obj_act[obj_act != (ob+1)] = 0\n",
    "    area = np.sum(obj_act>0) * dx_m[dx]**2\n",
    "    \n",
    "    if area < MinObjVolume:\n",
    "        obj_act[:] = 0\n",
    "        ii += 1\n",
    "    else:\n",
    "        jj += 1\n",
    "    dbz_objects[Objects[ob]] = dbz_objects[Objects[ob]] + obj_act\n",
    "    \n",
    "rgiObj_Struct=np.zeros((3,3,3)); rgiObj_Struct[:,:,:]=1\n",
    "dbz_objects, nr_objectsUD = ndimage.label(dbz_objects, structure=rgiObj_Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('        break up long living cores')\n",
    "from Tracking_Functions import BreakupObjects\n",
    "MS_objects, object_split = BreakupObjects(dbz_objects,\n",
    "                            3/6.,\n",
    "                            1/6.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(dbz[40], vmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tracking_Functions import calc_grid_distance_area\n",
    "from Tracking_Functions import calc_object_characteristics\n",
    "_,_,Area,Gridspacing = calc_grid_distance_area(lon,lat)\n",
    "\n",
    "cell_obj_characteristics = calc_object_characteristics(MS_objects.astype('int'), # feature object file\n",
    "                                     dbz,         # original file used for feature detection\n",
    "                                     'cell_stats',\n",
    "                                     rgdTimeMCS,            # timesteps of the data\n",
    "                                     lat,             # 2D latidudes\n",
    "                                     lon,             # 2D Longitudes\n",
    "                                     Gridspacing,\n",
    "                                     Area,\n",
    "                                     min_tsteps=3,\n",
    "                                     split_merge = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location of virtual sounders in grid\n",
    "from Tracking_Functions import radialdistance\n",
    "lat_vs_id = np.copy(lat_vs); lat_vs_id[:] = np.nan\n",
    "lon_vs_id = np.copy(lat_vs_id)\n",
    "for ii in tqdm(range(len(lat_vs))):\n",
    "    mindist = radialdistance(lat,lon,lat_vs[ii],lon_vs[ii])\n",
    "    min_loc = np.argwhere(mindist == np.min(mindist))[0]\n",
    "    lat_vs_id[ii] = min_loc[0]\n",
    "    lon_vs_id[ii] = min_loc[1]\n",
    "\n",
    "lat_vs_id = lat_vs_id.astype('int')\n",
    "lon_vs_id = lon_vs_id.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the location of cold cloud sheidls from anvile clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itb_threshold = 241 # K\n",
    "# MinObjVolume = (40 * 1000)**2 # m2\n",
    "# iSmooth = 16000\n",
    "# rgiObj_Struct=np.zeros((3,3,3)); rgiObj_Struct[1,:,:]=1\n",
    "\n",
    "# iSmoothGC=int(iSmooth/dx_m[dx])\n",
    "# tb_smooth=scipy.ndimage.uniform_filter(tb[:,:,:],[1,iSmoothGC,iSmoothGC])\n",
    "\n",
    "# # threshold the tb\n",
    "# rgiTH_tb=(tb_smooth <= itb_threshold)\n",
    "# rgrtb_thresholded=np.copy(tb)\n",
    "# rgrtb_thresholded[rgiTH_tb == False]=0\n",
    "# rgiObjectsUD, nr_objectsUD = ndimage.label(rgiTH_tb,structure=rgiObj_Struct)\n",
    "# Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "\n",
    "# # remove the objects that are smaller than the min. area and do not feature overshoots\n",
    "# mcs_areas = np.copy(rgiObjectsUD); mcs_areas[:] = 0\n",
    "# for ob in range(len(Objects)):\n",
    "#     sel_obj = np.copy(rgiObjectsUD[Objects[ob]])\n",
    "#     sel_obj[sel_obj != (ob +1)] = 0\n",
    "#     obj_area = np.sum(sel_obj > 0, axis=(1,2)) * dx_m[dx]**2\n",
    "#     big_enough = obj_area >= MinObjVolume\n",
    "#     sel_obj[big_enough == 0,:] = 0\n",
    "    \n",
    "#     mcs_areas[Objects[ob]] = mcs_areas[Objects[ob]] + sel_obj\n",
    "\n",
    "\n",
    "# # # sort the objects according to their size\n",
    "# # Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "# # rgiVolObj=np.array([np.sum(rgiObjectsUD[Objects[ob]] == ob+1) for ob in range(nr_objectsUD)])\n",
    "# # rgiObBySize=np.array([np.where(np.sort(rgiVolObj)[::-1][ob] == rgiVolObj)[0][0] for ob in range(nr_objectsUD)])\n",
    "# # rgiObBySize=rgiObBySize[rgiVolObj[rgiObBySize] >= MinObjVolume]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load draft data and calculate draft characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = 60000 # m -- distance to extract arround the station\n",
    "timewindow = 40 # minutes arround 2D obs\n",
    "dx_dist = int(dist/dx_m[dx])\n",
    "\n",
    "cores_vp_up = {}\n",
    "cores_vp_up_obs = {}\n",
    "cores_2d_up = {}\n",
    "cores_3d_up = {}\n",
    "cores_vp_down = {}\n",
    "cores_vp_obs = {}\n",
    "cores_2d_down = {}\n",
    "cores_3d_down = {}\n",
    "\n",
    "ud_min = 1.5\n",
    "dd_min = 1.5\n",
    "draft_th = [1.5,3.0,6.0,10.0]\n",
    "\n",
    "direction = ['up', 'down']  # [ up - updraft; down - downdraft]\n",
    "\n",
    "# loop over virtual profilers\n",
    "for ii in tqdm(range(len(lat_vs))):\n",
    "    conv_inters = np.unique(MS_objects[:,lat_vs_id[ii],lon_vs_id[ii]])[1:]\n",
    "    if len(conv_inters) == 0:\n",
    "        # this sounder does not intersect with an MCS\n",
    "        continue\n",
    "    la_start = lat_vs_id[ii] - dx_dist\n",
    "    la_stop = lat_vs_id[ii] + dx_dist + 1\n",
    "    lo_start = lon_vs_id[ii] - dx_dist\n",
    "    lo_stop = lon_vs_id[ii] + dx_dist +1\n",
    "\n",
    "    location = 'loc-'+str(ii).zfill(2)\n",
    "\n",
    "    if subkm == True:\n",
    "        # virtual profiler data\n",
    "        data_vp = grDATA[DX[dx]][:,:,ii,:]\n",
    "\n",
    "    for ca in range(len(conv_inters)):\n",
    "        conv_element = 'conv-'+str(ca).zfill(3)\n",
    "        files_int = wrfout_files[conv_inters[ca] == (MS_objects[:,lat_vs_id[ii],lon_vs_id[ii]])]\n",
    "        speed_ca_median = np.median(cell_obj_characteristics[str(conv_inters[ca])]['speed'])\n",
    "        for fi in [0]: #range(len(files_int)):\n",
    "            file_act = 'output-'+str(fi).zfill(2)\n",
    "            ncfile = Dataset(files_int[fi])\n",
    "            dbz_ca = np.squeeze(ncfile.variables[\"REFL_10CM\"][0,:,la_start:la_stop,lo_start:lo_stop])\n",
    "            w_ca = np.squeeze(ncfile.variables[\"W\"][0,:,la_start:la_stop,lo_start:lo_stop])\n",
    "            z_ca_stag = (np.squeeze(ncfile.variables[\"PHB\"][0,:,la_start:la_stop,lo_start:lo_stop]) + \\\n",
    "                    np.squeeze(ncfile.variables[\"PH\"][0,:,la_start:la_stop,lo_start:lo_stop])) / 9.81 - \\\n",
    "                    np.squeeze(ncfile.variables[\"HGT\"][0,la_start:la_stop,lo_start:lo_stop])\n",
    "            z_ca = (z_ca_stag[1:,:] + z_ca_stag[:-1,:])/2\n",
    "            # brind 3D data to common grid\n",
    "            dbz_ca_comz = wrf.interpz3d(dbz_ca, z_ca, height, missing = np.nan)\n",
    "            w_ca_comz = wrf.interpz3d(w_ca, z_ca_stag, height, missing = np.nan)\n",
    "            if skip_ml == '_ml-scip':\n",
    "                dbz_ca_comz[fl_layer,:] = np.nan\n",
    "                w_ca_comz[fl_layer,:] = np.nan\n",
    "            ncfile.close()\n",
    "            \n",
    "\n",
    "            # degrees to meters\n",
    "            lon_m = np.linspace(0,Gridspacing*w_ca.shape[1], w_ca.shape[1]); lon_m = lon_m - np.mean(lon_m)\n",
    "            lat_m = np.copy(lon_m)\n",
    "\n",
    "            cloudmask = np.copy(dbz_ca_comz[:,dx_dist,:]); cloudmask[:] = 1\n",
    "            cloudmask[0,:] = np.nan\n",
    "            cloudmask[-1,:] = np.nan\n",
    "            cloudmask[dbz_ca_comz[:,dx_dist,:] <= -30] = np.nan\n",
    "            \n",
    "            cloudmask_3d = np.copy(dbz_ca_comz[:,:,:]); cloudmask_3d[:] = 1\n",
    "            cloudmask_3d[dbz_ca_comz[:,:,:] <= -30] = np.nan\n",
    "\n",
    "#             plt.pcolormesh(lon_m/1000., height, w_ca_comz[:,dx_dist,:], vmin=-25, vmax=25, cmap='coolwarm')\n",
    "#             plt.contour(lon_m/1000., height, w_ca_comz[:,dx_dist,:] > 5, colors = 'r')\n",
    "#             plt.contour(lon_m/1000., height, w_ca_comz[:,dx_dist,:] < -5, colors = 'b')\n",
    "#             plt.contour(lon_m/1000., height, \n",
    "#                            cloudmask > 0,\n",
    "#                         colors='k')\n",
    "#             plt.show()\n",
    "            \n",
    "#             stop()\n",
    "\n",
    "            for di in range(len(direction)):\n",
    "                if direction[di] == 'up':\n",
    "                    dire_sign = 1\n",
    "                elif direction[di] == 'down':\n",
    "                    dire_sign = -1\n",
    "\n",
    "                for th in range(len(draft_th)):\n",
    "                    # WORKING ON 2D CORES\n",
    "                    updrafts = w_ca_comz[:,dx_dist,:]  * dire_sign > draft_th[th]\n",
    "                    rgiObjectsUD, nr_objectsUD = ndimage.label(updrafts,structure=rgiObj_Struct[0,:,:])\n",
    "\n",
    "                    w_2D = np.copy(w_ca_comz[:,dx_dist,:]) * dire_sign\n",
    "                    \n",
    "                    # watershedding\n",
    "                    rgiObjectsUD = watersheding(w_ca_comz[:,dx_dist,:]  * dire_sign, \n",
    "                                                 int(2000/dx_m[dx]),\n",
    "                                                 draft_th[th])\n",
    "                    # sort the objects according to their size\n",
    "                    Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "                                        \n",
    "                    if direction[di] == 'up':\n",
    "                        cores_2d_up[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+file_act] = core_2d_properties(Objects,\n",
    "                                  rgiObjectsUD,\n",
    "                                      w_2D,\n",
    "                                      cloudmask,\n",
    "                                      height,\n",
    "                                      lon_m)\n",
    "                    elif direction[di] == 'down':\n",
    "                        cores_2d_down[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+file_act] = core_2d_properties(Objects,\n",
    "                                  rgiObjectsUD,\n",
    "                                      w_2D,\n",
    "                                      cloudmask,\n",
    "                                      height,\n",
    "                                      lon_m)\n",
    "\n",
    "                        \n",
    "                    #  WORK ON 3D CORES\n",
    "                    updrafts = w_ca_comz[:,:,:]  * dire_sign > draft_th[th]\n",
    "                    rgiObjectsUD, nr_objectsUD = ndimage.label(updrafts,structure=rgiObj_Struct[:,:,:])\n",
    "                    if nr_objectsUD == 0:\n",
    "                        # no elements found\n",
    "                        continue\n",
    "                    \n",
    "                    w_3D = np.copy(w_ca_comz[:,:,:]) * dire_sign\n",
    "                    w_3D[np.isnan(cloudmask_3d)] = np.nan\n",
    "                    \n",
    "                    # watershedding\n",
    "                    rgiObjectsUD = watersheding(w_ca_comz[:,:,:]  * dire_sign, \n",
    "                                                 int(2000/dx_m[dx]),\n",
    "                                                 draft_th[th] * dire_sign)\n",
    "                    # sort the objects according to their size\n",
    "                    Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "\n",
    "                    rgiObjectsUD = rgiObjectsUD.astype(float)\n",
    "                    rgiObjectsUD[np.isnan(cloudmask_3d)] = np.nan\n",
    "\n",
    "                    if direction[di] == 'up':\n",
    "                        cores_3d_up[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+file_act] = core_3d_properties(w_3D,\n",
    "                                          rgiObjectsUD,\n",
    "                                          Objects,\n",
    "                                          height,\n",
    "                                          lat_m,\n",
    "                                          lon_m)\n",
    "                    elif direction[di] == 'down':\n",
    "                        cores_3d_down[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+file_act] = core_3d_properties(w_3D,\n",
    "                                          rgiObjectsUD,\n",
    "                                          Objects,\n",
    "                                          height,\n",
    "                                          lat_m,\n",
    "                                          lon_m)\n",
    "\n",
    "                    \n",
    "        if subkm == True:                \n",
    "            # ---------------------------------------------\n",
    "            # calculate properties from virtual profiler\n",
    "            time_ca = time_wrfout[conv_inters[ca] == (MS_objects[:,lat_vs_id[ii],lon_vs_id[ii]])]\n",
    "            time_vp = (rgdTimeMCS <= (time_ca + timedelta(minutes=timewindow))[0]) & \\\n",
    "                      (rgdTimeMCS >= (time_ca - timedelta(minutes=timewindow))[0])  \n",
    "            time_to_length = np.arange(0,sum(time_vp)*DT[dx], DT[dx]) * speed_ca_median\n",
    "\n",
    "            wrf_dbz_column = wrf.dbz(data_vp[time_vp,:,VARS.index('P')][None,:], \n",
    "                            data_vp[time_vp,:,VARS.index('TK')][None,:], \n",
    "                            data_vp[time_vp,:,VARS.index('QV')][None,:], \n",
    "                            data_vp[time_vp,:,VARS.index('QR')][None,:], \n",
    "                            qs=data_vp[time_vp,:,VARS.index('QS')][None,:], \n",
    "                            qg=data_vp[time_vp,:,VARS.index('QG')][None,:],\n",
    "                            use_varint = True,\n",
    "                            use_liqskin=True)\n",
    "\n",
    "            # plt.pcolormesh(time_to_length/1000., height, data_vp[time_vp,:,VARS.index('WW')].T, vmin=-25, vmax=25, cmap='coolwarm')\n",
    "            # plt.contour(time_to_length/1000., height, data_vp[time_vp,:,VARS.index('WW')].T > draft_th[th], colors = 'r')\n",
    "            # plt.contour(time_to_length/1000., height, data_vp[time_vp,:,VARS.index('WW')].T < -draft_th[th], colors = 'b')\n",
    "            # plt.contour(time_to_length/1000., height, \n",
    "            #                cloudmask[:,:].T > 0, \n",
    "            #             colors='k')\n",
    "            # plt.show()\n",
    "\n",
    "            for th in range(len(draft_th)):\n",
    "                for di in [1]: #range(len(direction)):\n",
    "                    if direction[di] == 'up':\n",
    "                        dire_sign = 1\n",
    "                    elif direction[di] == 'down':\n",
    "                        dire_sign = -1\n",
    "\n",
    "                    # virtual profiler draft characteristics\n",
    "                    updrafts = data_vp[time_vp,:,VARS.index('WW')].T * dire_sign\n",
    "\n",
    "                    # we have to aggregate the temporal data to a similar resolution as the spatial\n",
    "                    # data to get the same watersheding characteristics\n",
    "                    intervals = int(time_to_length.max()/dx_m[dx])\n",
    "                    bins_per_int = int(updrafts.shape[1]/intervals)\n",
    "                    updrafts = np.copy(updrafts[:, :intervals * bins_per_int])\n",
    "                    updrafts = np.mean(np.reshape(updrafts, \n",
    "                                              (updrafts.shape[0], intervals, bins_per_int)), axis = 2)\n",
    "                    updrafts_th = updrafts > draft_th[th]\n",
    "\n",
    "                    cloudmask = np.copy(DATA[time_vp,:,ii,VARS.index('WW')]); cloudmask[:] = 1\n",
    "                    cloudmask[0,:] = np.nan\n",
    "                    cloudmask[-1,:] = np.nan\n",
    "                    cloudmask[wrf_dbz_column[0] <= -30] = np.nan\n",
    "                    cloudmask = np.copy(cloudmask.T[:, :intervals * bins_per_int])\n",
    "                    cloudmask = np.mean(np.reshape(cloudmask, \n",
    "                                              (cloudmask.shape[0], intervals, bins_per_int)), axis = 2)\n",
    "\n",
    "                    rgiObjectsUD, nr_objectsUD = ndimage.label(updrafts_th,structure=rgiObj_Struct[0,:,:])\n",
    "                    # watershedding\n",
    "                    rgiObjectsUD = watersheding(updrafts, \n",
    "                                                 int(2000/dx_m[dx]),\n",
    "                                                 draft_th[th])\n",
    "                    # sort the objects according to their size\n",
    "                    Objects=ndimage.find_objects(rgiObjectsUD)\n",
    "\n",
    "                    # w_2D = np.copy(data_vp[time_vp,:,VARS.index('WW')].T)* dire_sign\n",
    "                    # # get w_2D to dx resolution\n",
    "                    # w_2D = np.copy(w_2D[:, :intervals * bins_per_int])\n",
    "                    # updrafts = np.mean(np.reshape(updrafts, \n",
    "                    #                           (updrafts.shape[0], intervals, bins_per_int)), axis = 2)\n",
    "                    if direction[di] == 'up':\n",
    "                        cores_vp_up[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+str(ii)] = core_2d_properties(Objects,\n",
    "                                      rgiObjectsUD,\n",
    "                                      updrafts,\n",
    "                                      cloudmask,\n",
    "                                      height,\n",
    "                                      time_to_length[::bins_per_int][:rgiObjectsUD.shape[1]])\n",
    "                    elif direction[di] == 'down':\n",
    "                        cores_vp_down[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+str(ii)] = core_2d_properties(Objects,\n",
    "                                      rgiObjectsUD,\n",
    "                                      updrafts,\n",
    "                                      cloudmask,\n",
    "                                      height,\n",
    "                                      time_to_length[::bins_per_int][:rgiObjectsUD.shape[1]])\n",
    "                        stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores_vp_down['1.5_loc-00_conv-000_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores_2d_down[str(draft_th[th]).zfill(2)+'_'+location+'_'+conv_element+'_'+file_act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(rgiObjectsUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updrafts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_stats = {\n",
    "                    \"cores_2d_up\": cores_2d_up,\n",
    "                    \"cores_vp_up\": cores_vp_up,\n",
    "                    \"cores_3d_up\": cores_3d_up,\n",
    "                    \"cores_2d_down\": cores_2d_down,\n",
    "                    \"cores_vp_down\": cores_vp_down,\n",
    "                    \"cores_3d_down\": cores_3d_down,\n",
    "                    \"speed_ca_median\": speed_ca_median,\n",
    "                }\n",
    "\n",
    "with open(outdir + SIM_All[si] +'_'+ DX[dx] +'.pkl', 'wb') as handle:\n",
    "    pickle.dump(draft_stats, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr_core_act['mean elevation'] = mean_height\n",
    "# gr_core_act['mean depth'] = core_height_mean\n",
    "# gr_core_act['max depth'] = core_heigth_max\n",
    "# gr_core_act['mean speed'] = core_speed_mean\n",
    "# gr_core_act['max speed'] = core_speed_max\n",
    "# gr_core_act['mean width'] = core_with_mean\n",
    "# gr_core_act['max width'] = core_with_max\n",
    "# gr_core_act['slope from vertical'] = core_slope\n",
    "\n",
    "stat = 'mean elevation'\n",
    "th = '1.5' # [1.5,3.0,6.0,10.0]\n",
    "cores_2d = cores_2d_up\n",
    "cores_vp = cores_vp_up\n",
    "cores_3d = cores_3d_up\n",
    "\n",
    "# cores_2d = cores_2d_down\n",
    "# cores_vp = cores_vp_down\n",
    "# cores_3d = cores_3d_down\n",
    "\n",
    "\n",
    "\n",
    "mean_width_2d = [\n",
    "                    [\n",
    "                        cores_2d[ii][jj][stat]\n",
    "                        for jj in cores_2d[ii].keys()\n",
    "                    ]\n",
    "                    for ii in [s for s in list(cores_2d.keys()) if th in s]\n",
    "                ]\n",
    "mean_width_2d = np.array([x  for sublist in mean_width_2d for x in sublist])\n",
    "\n",
    "mean_width_vp = [\n",
    "                    [\n",
    "                        cores_vp[ii][jj][stat]\n",
    "                        for jj in cores_vp[ii].keys()\n",
    "                    ]\n",
    "                    for ii in [s for s in list(cores_vp.keys()) if th in s]\n",
    "                ]\n",
    "mean_width_vp = np.array([x  for sublist in mean_width_vp for x in sublist])\n",
    "\n",
    "mean_width_3d = [\n",
    "                    [\n",
    "                        np.mean(cores_3d[ii][jj][stat])\n",
    "                        for jj in cores_3d[ii].keys()\n",
    "                    ]\n",
    "                    for ii in [s for s in list(cores_3d.keys()) if th in s]\n",
    "                ]\n",
    "mean_width_3d = np.array([x  for sublist in mean_width_3d for x in sublist])\n",
    "\n",
    "try:\n",
    "    mean_width_3d = np.concatenate(mean_width_3d).ravel().tolist()\n",
    "except:\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "if stat == 'mean elevation':\n",
    "    newBins = np.arange(0,20000,500)\n",
    "elif stat == 'mean width':\n",
    "    newBins = np.arange(0,15000,1000)\n",
    "elif stat == 'mean speed':\n",
    "    newBins = np.arange(0,20,1)\n",
    "elif stat == 'max speed':\n",
    "    newBins = np.arange(0,20,1)\n",
    "    \n",
    "fig = sns.distplot(mean_width_2d, color=\"k\", label='2D', bins=newBins)\n",
    "sns.distplot(mean_width_vp, color=\"r\", label='virtual profiler', bins=newBins)\n",
    "# sns.distplot( a=mean_width_vp, hist=True, kde=False, rug=False )\n",
    "sns.distplot(mean_width_3d, color=\"b\", label='3D', bins=newBins)\n",
    "plt.xlabel(stat)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
